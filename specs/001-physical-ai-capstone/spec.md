---
title: "Physical AI — Capstone Quarter"
sidebar_position: 1
slug: "/physical-ai-capstone"
id: "physical-ai-capstone"
---

<!-- toc -->

## Overview

Physical AI represents the convergence of artificial intelligence with physical systems, enabling robots to understand and interact with the physical world. This capstone quarter focuses on developing embodied intelligence through humanoid robotics, combining perception, planning, and action in real-world environments.

## Modules

### Module 1: Foundations of Physical AI
*Understanding embodied intelligence and physical laws.*
- Core principles of embodied cognition
- Physics simulation and real-world transfer
- Sensor fusion and perception systems

### Module 2: ROS 2 and Robotic Control
*Mastering the Robot Operating System for robot development.*
- ROS 2 architecture and core concepts
- Node communication and message passing
- Building robotic applications with Python

### Module 3: Simulation and Development
*Using industry-standard tools for robot simulation.*
- Gazebo physics simulation environment
- Unity for robot visualization
- NVIDIA Isaac SDK and Isaac Sim

### Module 4: Vision-Language-Action (VLA)
*The convergence of LLMs and Robotics.*
- Voice-to-Action: Using OpenAI Whisper for voice commands
- Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions
- Capstone Project: The Autonomous Humanoid — a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it

## Why Physical AI Matters

Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.

## Learning Outcomes

1. Understand Physical AI principles and embodied intelligence
2. Master ROS 2 for robotic control
3. Simulate robots with Gazebo and Unity
4. Develop with NVIDIA Isaac
5. Design humanoid robots for natural interactions
6. Integrate GPT models for conversational robotics

## Weekly Breakdown

#### Weeks 1–2
• Foundations of Physical AI and embodied intelligence
• From digital AI to robots that understand physical laws
• Overview of humanoid robotics landscape
• Sensor systems: LIDAR, cameras, IMUs, force/torque sensors

#### Weeks 3–5
• ROS 2 architecture and core concepts
• Nodes, topics, services, and actions
• Building ROS 2 packages with Python
• Launch files and parameter management

#### Weeks 6–7
• Gazebo simulation environment setup
• URDF and SDF formats
• Physics simulation and sensor simulation
• Introduction to Unity for robot visualization

#### Weeks 8–10
• NVIDIA Isaac SDK and Isaac Sim
• AI-powered perception and manipulation
• Reinforcement learning for robot control
• Sim-to-real transfer techniques

#### Weeks 11–12
• Humanoid robot kinematics and dynamics
• Bipedal locomotion and balance control
• Manipulation and grasping
• Natural human-robot interaction design

#### Week 13
• Integrating GPT models in robots
• Speech recognition
• Multi-modal interaction: speech, gesture, vision

## Capstone Project

The Autonomous Humanoid project integrates all concepts learned throughout the quarter. Students will develop a simulated humanoid robot that can receive voice commands, plan paths, navigate obstacles, identify objects using computer vision, and manipulate them using robotic arms. The project demonstrates the full stack of Physical AI capabilities from perception to action.
